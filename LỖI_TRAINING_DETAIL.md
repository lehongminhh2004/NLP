# ğŸ› Lá»–I TRONG TRAINING VÃ€ CÃCH FIX

## **ğŸ”´ Lá»–I: ValueError - not enough values to unpack (expected 3, got 2)**

### **1. Vá»Š TRÃ Lá»–I**
Cell 14 - Encoder class - method `forward()`

### **2. NGUYÃŠN NHÃ‚N**

**Code cÅ© (SAIIII):**
```python
def forward(self, src, src_lengths, return_outputs=False):
    # ...
    packed_output, (hidden, cell) = self.lstm(packed)
    
    _, (hidden, cell) = self.lstm(packed)  # âŒ Cháº¡y LSTM Láº¦N 2!
    return hidden, cell  # âŒ Return chá»‰ 2 giÃ¡ trá»‹
```

**Váº¥n Ä‘á»:**
- Cháº¡y LSTM 2 láº§n â†’ lÃ£ng phÃ­ tÃ­nh toÃ¡n
- Return chá»‰ 2 giÃ¡ trá»‹ (hidden, cell)
- NhÆ°ng training code cá»‘ unpack 3 giÃ¡ trá»‹:
  ```python
  hidden, cell, _ = self.encoder(src, src_lengths, return_outputs=False)
  ```
  â†’ **ValueError: not enough values to unpack (expected 3, got 2)**

---

### **3. GIáº¢I PHÃP**

**Code má»›i (ÄÃšNG):**
```python
def forward(self, src, src_lengths, return_outputs=False):
    # src: [B, src_len], src_lengths: [B] (sorted desc)
    embedded = self.dropout(self.embedding(src))  # [B, src_len, emb_dim]

    packed = pack_padded_sequence(
        embedded,
        lengths=src_lengths.cpu(),
        batch_first=True,
        enforce_sorted=True
    )

    packed_output, (hidden, cell) = self.lstm(packed)
    
    # âœ… Chá»‰ unpack náº¿u cáº§n (cho attention model)
    if return_outputs:
        from torch.nn.utils.rnn import pad_packed_sequence
        encoder_outputs, _ = pad_packed_sequence(packed_output, batch_first=True)
        return hidden, cell, encoder_outputs  # âœ… Return 3 giÃ¡ trá»‹
    
    return hidden, cell, None  # âœ… Return 3 giÃ¡ trá»‹ (giÃ¡ trá»‹ cuá»‘i lÃ  None)
```

---

## **âœ… THAY Äá»”I ÄÃƒ ÄÆ¯á»¢C THá»°C HIá»†N**

âœ“ XÃ³a dÃ²ng LSTM cháº¡y láº§n 2 (dÃ²ng `_, (hidden, cell) = self.lstm(packed)`)
âœ“ ThÃªm logic `if return_outputs` Ä‘á»ƒ unpack khi cáº§n
âœ“ Return **luÃ´n 3 giÃ¡ trá»‹** (hidden, cell, encoder_outputs hoáº·c None)
âœ“ TÆ°Æ¡ng thÃ­ch vá»›i training code: `hidden, cell, _ = self.encoder(...)`
âœ“ TÆ°Æ¡ng thÃ­ch vá»›i Attention model: sá»­ dá»¥ng `encoder_outputs` khi cáº§n

---

## **ğŸ“ DÃ’NG CÃ“ Lá»–I CHI TIáº¾T**

| DÃ²ng | Code | Lá»—i |
|------|------|-----|
| 391-392 | `packed_output, (hidden, cell) = self.lstm(packed)`  `_, (hidden, cell) = self.lstm(packed)` | âŒ Cháº¡y LSTM 2 láº§n |
| 393 | `return hidden, cell` | âŒ Return 2 giÃ¡ trá»‹, nhÆ°ng code cáº§u 3 |

---

## **ğŸ¯ Káº¾T QUáº¢ SAU KHI FIX**

Encoder sáº½:
- âœ… Cháº¡y LSTM **Ä‘Ãºng 1 láº§n** (tiáº¿t kiá»‡m 50% computation)
- âœ… Return **3 giÃ¡ trá»‹ luÃ´n** (hidden, cell, encoder_outputs/None)
- âœ… TÆ°Æ¡ng thÃ­ch vá»›i Seq2Seq baseline
- âœ… TÆ°Æ¡ng thÃ­ch vá»›i Seq2SeqWithAttention

---

## **TEST Láº I**

BÃ¢y giá» hÃ£y cháº¡y láº¡i Cell 22 (Training) xem lá»—i cÃ³ biáº¿n máº¥t khÃ´ng! ğŸš€

