# ‚ö° PH√ÇN T√çCH V√Ä T·ªêI ∆ØU PERFORMANCE - TRAINING CH·∫¨M

## I. C√ÅC V·∫§N ƒê·ªÄ CH√çNH G·∫†Y CH·∫¨M

### üî¥ **1. COLLATE_FN - Sorting Each Batch (MAJOR)**

**V·∫•n ƒë·ªÅ:**
```python
def collate_fn(batch, pad_idx=0):
    src_list, trg_list = zip(*batch)
    src_lengths = torch.tensor([len(s) for s in src_list], dtype=torch.long)
    
    # ‚ö†Ô∏è BOTTLENECK: Sorting m·ªói batch O(B log B)
    src_lengths, sort_idx = src_lengths.sort(descending=True)
    src_batch = src_batch[sort_idx]
    trg_batch = trg_batch[sort_idx]
    
    return src_batch, src_lengths, trg_batch
```

**T√°c ƒë·ªông:**
- M·ªói batch (64 samples) ph·∫£i sort ‚Üí **O(64 log 64) = ~384 operations**
- Train: 29,000 samples ‚Üí ~453 batches/epoch
- **Total: ~175,392 sort operations m·ªói epoch** üê¢

**Gi·∫£i ph√°p:**
1. **Bucket Sampler**: Nh√≥m c√¢u theo ƒë·ªô d√†i TR∆Ø·ªöC khi v√†o DataLoader
2. **Kh√¥ng sort**: D·ªØ li·ªáu ƒë√£ pre-sorted
3. Ho·∫∑c d√πng `BucketBatchSampler`

---

### üî¥ **2. DataLoader - Thi·∫øu Optimization (MAJOR)**

**V·∫•n ƒë·ªÅ:**
```python
train_loader = DataLoader(train_dataset, 
                         batch_size=BATCH_SIZE, 
                         shuffle=True,  
                         collate_fn=collate_fn)
# ‚ùå Thi·∫øu: num_workers, pin_memory, prefetch_factor
```

**Thi·∫øu optimizations:**
| Tham s·ªë | Hi·ªán t·∫°i | Khuy·∫øn ngh·ªã | L·ª£i √≠ch |
|---------|----------|------------|---------|
| `num_workers` | 0 (main thread) | 4-8 | Parallel data loading |
| `pin_memory` | False | True | Faster GPU transfer |
| `prefetch_factor` | 2 | 4-8 | Pre-load batches |
| `persistent_workers` | N/A | True | T√°i s·ª≠ d·ª•ng processes |

**T√°c ƒë·ªông:**
- M·ªói batch ph·∫£i load t·ª´ CPU ‚Üí GPU sequentially
- Kh√¥ng parallel data processing
- GPU ch·ªù data trong khi CPU load ‚Üí **GPU idle time l·ªõn**

**T√≠nh to√°n:**
- Batch load time: ~0.5s
- 453 batches/epoch √ó 0.5s = **~226 seconds** cho ch·ªâ data loading! üò±

---

### üü† **3. Model Architecture - Qu√° Ph·ª©c T·∫°p (MEDIUM)**

**Hi·ªán t·∫°i:**
```python
EMBEDDING_DIM = 256       # 256 dimensions
HIDDEN_DIM = 512          # 512 hidden units
N_LAYERS = 2              # 2 LSTM layers
```

**S·ªë parameters:**
```
Encoder:
  - Embedding: 10,000 vocab √ó 256 = 2.56M
  - LSTM (2 layers): 256 + 512 √ó 4 √ó 512 √ó 2 = 4.19M
  Total: ~6.75M

Decoder:
  - Embedding: 10,000 √ó 256 = 2.56M
  - LSTM: 4.19M
  Total: ~6.75M

Total Model: ~13.5M parameters
```

**T√°c ƒë·ªông:**
- M·ªói forward pass: 13.5M √ó 4 bytes = 54MB memory
- Backward pass: 54MB √ó 2 = 108MB gradient
- **T·ªïng: ~162MB per batch** (ch∆∞a t√≠nh optimizer state)

**Gi·∫£i ph√°p:**
- Gi·∫£m HIDDEN_DIM t·ª´ 512 ‚Üí 256 (gi·∫£m 75% computation)
- Ho·∫∑c gi·∫£m EMBEDDING_DIM t·ª´ 256 ‚Üí 128

---

### üü† **4. Pack/Unpack Sequence - C√≥ Overhead (MEDIUM)**

**V·∫•n ƒë·ªÅ:**
```python
packed = pack_padded_sequence(embedded, lengths=src_lengths, 
                              batch_first=True, enforce_sorted=True)
packed_output, (hidden, cell) = self.lstm(packed)
encoder_outputs, _ = pad_packed_sequence(packed_output, batch_first=True)
```

**T√°c ƒë·ªông:**
- Pack: Create mask + sparse data structure
- LSTM: Ch·∫°y tr√™n sparse data (n√£o efficient cho CUDA)
- Unpack: Restore padding
- **Overhead: ~10-15% t√≠nh to√°n**

**Khi n√†o h·ªØu d·ª•ng:**
- ‚úÖ Khi variance ƒë·ªô d√†i l·ªõn (v√≠ d·ª•: 5 ‚Üí 100 tokens)
- ‚ùå Khi variance nh·ªè (v√≠ d·ª•: 50 ‚Üí 65 tokens)

---

### üü° **5. Teacher Forcing - Random Check M·ªói Step (MINOR)**

**V·∫•n ƒë·ªÅ:**
```python
for t in range(1, trg_len):
    out, hidden, cell = self.decoder(inp, hidden, cell)
    outputs[:, t, :] = out
    
    # ‚ö†Ô∏è Random number generation m·ªói timestep
    teacher_force = random.random() < teacher_forcing_ratio
    top1 = out.argmax(1)
    inp = trg[:, t] if teacher_force else top1
```

**T√°c ƒë·ªông:**
- M·ªói target token (trung b√¨nh ~20): `random.random()` call
- 453 batches √ó 64 samples √ó 20 tokens √ó random() = **579,840 random calls/epoch**
- ~2-3% overhead

**Gi·∫£i ph√°p:**
- Pre-generate random masks tr∆∞·ªõc
- Vectorize: `teacher_force = torch.rand(B) < ratio`

---

### üü° **6. Kh√¥ng S·ª≠ D·ª•ng Mixed Precision (MINOR)**

**V·∫•n ƒë·ªÅ:**
```python
# ‚ùå T·∫•t c·∫£ computation s·ª≠ d·ª•ng float32
output = model(src, src_lengths, trg, ...)
loss = criterion(output, trg)
```

**C√≥ th·ªÉ c·∫£i thi·ªán:**
- Float32 computation ch·∫≠m h∆°n float16
- GPU (nh·∫•t l√† RTX series) t·ªëi ∆∞u cho float16

**T√°c ƒë·ªông:**
- ~1.5-2x speedup n·∫øu d√πng `torch.autocast()`

---

## II. QUICK FIXES (√Åp d·ª•ng ngay)

### ‚úÖ **Fix 1: T·ªëi ∆∞u DataLoader**

```python
train_loader = DataLoader(
    train_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=4,              # ‚úÖ Parallel loading
    pin_memory=True,            # ‚úÖ GPU memory pinning
    prefetch_factor=4,          # ‚úÖ Pre-load batches
    persistent_workers=True,    # ‚úÖ Reuse workers
    drop_last=True              # ‚úÖ Avoid small last batch
)
```

**D·ª± ki·∫øn speedup: ~40-50%** ‚ö°

---

### ‚úÖ **Fix 2: Gi·∫£m Model Size**

```python
# Before
EMBEDDING_DIM = 256
HIDDEN_DIM = 512

# After
EMBEDDING_DIM = 128  # ‚úÖ Gi·∫£m 50%
HIDDEN_DIM = 256     # ‚úÖ Gi·∫£m 50%
```

**D·ª± ki·∫øn speedup: ~60-70%** ‚ö°‚ö°

---

### ‚úÖ **Fix 3: Vectorize Teacher Forcing**

```python
# Before
for t in range(1, trg_len):
    out, hidden, cell = self.decoder(inp, hidden, cell)
    teacher_force = random.random() < teacher_forcing_ratio  # ‚ùå Slow
    inp = trg[:, t] if teacher_force else out.argmax(1)

# After
def forward_with_teacher_forcing(self, src, src_lengths, trg, tf_ratio=0.5):
    # Generate mask m·ªôt l·∫ßn
    max_len = trg.size(1)
    tf_mask = torch.rand(trg.size(0), max_len) < tf_ratio  # [B, T]
    
    for t in range(1, max_len):
        out, hidden, cell = self.decoder(inp, hidden, cell)
        # Vectorized
        inp = torch.where(tf_mask[:, t], trg[:, t], out.argmax(1))
```

**D·ª± ki·∫øn speedup: ~2-3%** (nh·ªè nh∆∞ng cleaner)

---

### ‚úÖ **Fix 4: Enable Mixed Precision**

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

def train_epoch(model, iterator, optimizer, criterion, clip, device, tf_ratio):
    model.train()
    epoch_loss = 0.0
    
    for src, src_lengths, trg in iterator:
        src, trg = src.to(device), trg.to(device)
        optimizer.zero_grad()
        
        # ‚úÖ Use mixed precision
        with autocast():
            output = model(src, src_lengths, trg, teacher_forcing_ratio=tf_ratio)
            loss = criterion(output.reshape(-1, output.size(-1)), 
                           trg[:, 1:].reshape(-1))
        
        scaler.scale(loss).backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        scaler.step(optimizer)
        scaler.update()
        
        epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)
```

**D·ª± ki·∫øn speedup: ~1.3-1.8x** ‚ö°‚ö°‚ö°

---

## III. ADVANCED FIXES (C·∫£i thi·ªán s√¢u)

### üöÄ **Advanced 1: Bucket Sampler (Lo·∫°i Sorting)**

```python
class BucketBatchSampler:
    """Nh√≥m sequences theo length bucket ‚Üí kh√¥ng c·∫ßn sort/batch"""
    def __init__(self, data_lengths, batch_size, num_buckets=10):
        # Pre-sort data theo length
        # Nh√≥m v√†o buckets
        # M·ªói batch t·ª´ c√πng bucket (variance nh·ªè)
    
    def __iter__(self):
        for bucket in self.buckets:
            for i in range(0, len(bucket), self.batch_size):
                yield bucket[i:i+self.batch_size]

# Usage
sampler = BucketBatchSampler(src_lengths, batch_size=64, num_buckets=10)
train_loader = DataLoader(train_dataset, batch_sampler=sampler, 
                         num_workers=4, pin_memory=True)
```

**D·ª± ki·∫øn speedup: ~20-30%** (lo·∫°i sorting overhead)

---

### üöÄ **Advanced 2: Compile Model (PyTorch 2.0+)**

```python
if torch.__version__ >= "2.0":
    model = torch.compile(model, mode="reduce-overhead")
    # ‚úÖ JIT compile model
```

**D·ª± ki·∫øn speedup: ~1.2-1.5x**

---

### üöÄ **Advanced 3: Gradient Accumulation (n·∫øu OOM)**

```python
accumulation_steps = 4

for epoch in range(N_EPOCHS):
    for batch_idx, (src, src_lengths, trg) in enumerate(train_loader):
        output = model(src, src_lengths, trg, tf_ratio=TEACHER_FORCING_RATIO)
        loss = criterion(output.reshape(-1, output.size(-1)), 
                        trg[:, 1:].reshape(-1))
        
        loss.backward()
        
        # Update m·ªói 4 batches (nh∆∞ng data loading parallel)
        if (batch_idx + 1) % accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)
            optimizer.step()
            optimizer.zero_grad()
```

**Benefit:** Fit larger batches m√† kh√¥ng OOM

---

## IV. QUICK DIAGNOSTIC

ƒê·ªÉ x√°c ƒë·ªãnh bottleneck ch√≠nh, th√™m code n√†y:

```python
import time

def train_epoch_with_profiling(model, iterator, optimizer, criterion, 
                               clip, device, tf_ratio):
    model.train()
    epoch_loss = 0.0
    
    data_load_time = 0
    forward_time = 0
    backward_time = 0
    
    start_data = time.time()
    
    for src, src_lengths, trg in tqdm(iterator):
        data_load_time += time.time() - start_data
        src, trg = src.to(device), trg.to(device)
        
        # Forward
        start_forward = time.time()
        output = model(src, src_lengths, trg, teacher_forcing_ratio=tf_ratio)
        V = output.size(-1)
        
        output = output[:, 1:, :].reshape(-1, V)
        trg = trg[:, 1:].reshape(-1)
        loss = criterion(output, trg)
        forward_time += time.time() - start_forward
        
        # Backward
        start_backward = time.time()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        optimizer.zero_grad()
        backward_time += time.time() - start_backward
        
        epoch_loss += loss.item()
        start_data = time.time()
    
    total_time = data_load_time + forward_time + backward_time
    print(f"\n‚è±Ô∏è  Profiling Results:")
    print(f"  ‚Ä¢ Data Loading:  {data_load_time:.2f}s ({data_load_time/total_time*100:.1f}%)")
    print(f"  ‚Ä¢ Forward Pass:  {forward_time:.2f}s ({forward_time/total_time*100:.1f}%)")
    print(f"  ‚Ä¢ Backward Pass: {backward_time:.2f}s ({backward_time/total_time*100:.1f}%)")
    print(f"  ‚Ä¢ Total Time:    {total_time:.2f}s")
    
    return epoch_loss / len(iterator)
```

---

## V. RECOMMENDED OPTIMIZATION STRATEGY

**Th·ª© t·ª± √°p d·ª•ng (t·ª´ d·ªÖ ‚Üí kh√≥):**

| Th·ª© t·ª± | Fix | Speedup | ƒê·ªô kh√≥ | Th·ªùi gian |
|--------|-----|---------|--------|-----------|
| 1Ô∏è‚É£ | DataLoader (num_workers) | 40-50% | ‚≠ê | 2 ph√∫t |
| 2Ô∏è‚É£ | Gi·∫£m model size | 60-70% | ‚≠ê | 5 ph√∫t |
| 3Ô∏è‚É£ | Mixed Precision | 1.3-1.8x | ‚≠ê‚≠ê | 10 ph√∫t |
| 4Ô∏è‚É£ | Bucket Sampler | 20-30% | ‚≠ê‚≠ê | 30 ph√∫t |
| 5Ô∏è‚É£ | torch.compile | 1.2-1.5x | ‚≠ê‚≠ê | 5 ph√∫t |

**T·ªïng h·ª£p: ~2-3x t·ªïng speedup v·ªõi v√†i b∆∞·ªõc ƒë∆°n gi·∫£n!** üöÄ

---

## VI. EXAMPLE: OPTIMIZED CODE

```python
from torch.cuda.amp import autocast, GradScaler

# 1. T·ªëi ∆∞u DataLoader
train_loader = DataLoader(
    train_dataset, 
    batch_size=64,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=4,
    pin_memory=True,
    prefetch_factor=4,
    persistent_workers=True,
)

# 2. Gi·∫£m model size
EMBEDDING_DIM = 128   # t·ª´ 256
HIDDEN_DIM = 256      # t·ª´ 512

encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)
decoder = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)
model = Seq2Seq(encoder, decoder, device).to(device)

# 3. Mixed precision
scaler = GradScaler()

# 4. Training loop
def train_epoch_optimized(model, iterator, optimizer, criterion, clip, device, tf_ratio):
    model.train()
    epoch_loss = 0.0
    
    for src, src_lengths, trg in tqdm(iterator):
        src, trg = src.to(device), trg.to(device)
        optimizer.zero_grad()
        
        # Mixed precision
        with autocast():
            output = model(src, src_lengths, trg, teacher_forcing_ratio=tf_ratio)
            V = output.size(-1)
            loss = criterion(output[:, 1:, :].reshape(-1, V), 
                           trg[:, 1:].reshape(-1))
        
        scaler.scale(loss).backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        scaler.step(optimizer)
        scaler.update()
        
        epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)
```

**D·ª± ki·∫øn:**
- Tr∆∞·ªõc: 30 ph√∫t/epoch
- Sau: ~10-15 ph√∫t/epoch ‚ö°‚ö°‚ö°

---

