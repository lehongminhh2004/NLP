# ğŸ“Š PHÃ‚N TÃCH Cáº¤U TRÃšC & KIá»‚M TRA CODE TRAINING

## I. Cáº¤U TRÃšC Tá»”NG QUAN NOTEBOOK

Notebook Ä‘Æ°á»£c chia thÃ nh **17 pháº§n chÃ­nh**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHáº¦N 1: CÃ€I Äáº¶T (Cell 4)                  â”‚
â”‚  â€¢ Import thÆ° viá»‡n (torch, spacy, tqdm...)  â”‚
â”‚  â€¢ Kiá»ƒm tra device (CPU/GPU)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â¬‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHáº¦N 2: Táº¢I DATASET (Cell 6)               â”‚
â”‚  â€¢ Download Multi30K (EN-FR)                â”‚
â”‚  â€¢ Láº¥y train/val/test splits                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â¬‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHáº¦N 3-4: TIá»€N Xá»¬ LÃ (Cells 8, 10)        â”‚
â”‚  â€¢ Tokenization (spaCy)                     â”‚
â”‚  â€¢ Build Vocabulary vá»›i freq threshold      â”‚
â”‚  â€¢ Numericalization                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â¬‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHáº¦N 5: DATASET & DATALOADER (Cell 12)    â”‚
â”‚  â€¢ TranslationDataset class                 â”‚
â”‚  â€¢ collate_fn (padding + sorting by length) â”‚
â”‚  â€¢ Batch size = 64                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â¬‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHáº¦N 6: MODEL ARCHITECTURE (Cells 14-18)  â”‚
â”‚  â€¢ Encoder (LSTM + packing)                 â”‚
â”‚  â€¢ Decoder (LSTM)                           â”‚
â”‚  â€¢ Seq2Seq (attention-ready)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â¬‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHáº¦N 7-8: TRAINING (Cells 20, 22) â­      â”‚
â”‚  â€¢ train_epoch() function                   â”‚
â”‚  â€¢ evaluate() function                      â”‚
â”‚  â€¢ Training loop vá»›i early stopping         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â¬‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHáº¦N 9-13: ÄÃNH GIÃ & INFERENCE (Cells 24-32) â”‚
â”‚  â€¢ Plot training curves                     â”‚
â”‚  â€¢ Inference function                       â”‚
â”‚  â€¢ Calculate BLEU score                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â¬‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHáº¦N 14-17: ATTENTION MODEL (Cells 40-46) â”‚
â”‚  â€¢ Attention mechanism                      â”‚
â”‚  â€¢ DecoderWithAttention                     â”‚
â”‚  â€¢ Seq2SeqWithAttention                     â”‚
â”‚  â€¢ Training & Comparison                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## II. KIá»‚M TRA PHáº¦N TRAINING âœ…

### 2.1 HÃ m `train_epoch()` (Cell 20)

```python
def train_epoch(model, iterator, optimizer, criterion, clip, device, tf_ratio):
    model.train()                    # âœ… Set model to training mode
    epoch_loss = 0.0

    for src, src_lengths, trg in tqdm(iterator, ...):
        src, trg = src.to(device), trg.to(device)  # âœ… Move to device
        
        optimizer.zero_grad()        # âœ… Clear gradients
        output = model(src, src_lengths, trg, 
                      teacher_forcing_ratio=tf_ratio)  # [B, T, V]
        
        # âœ… Reshape Ä‘á»ƒ tÃ­nh loss (loáº¡i bá» <sos>)
        output = output[:, 1:, :].reshape(-1, V)
        trg    = trg[:, 1:].reshape(-1)
        
        loss = criterion(output, trg)  # âœ… TÃ­nh loss
        loss.backward()                # âœ… Backpropagation
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # âœ… Clip gradient
        optimizer.step()               # âœ… Update weights
        
        epoch_loss += loss.item()      # âœ… Accumulate loss
    
    return epoch_loss / len(iterator)  # âœ… Return average loss
```

**âœ… Äiá»ƒm tá»‘t:**
- âœ“ `model.train()` - Ä‘áº£m báº£o dropout & batch norm hoáº¡t Ä‘á»™ng
- âœ“ `optimizer.zero_grad()` - xÃ³a gradient cÅ© trÆ°á»›c backward
- âœ“ `clip_grad_norm_()` - trÃ¡nh exploding gradient
- âœ“ Reshape output Ä‘á»ƒ loáº¡i bá» `<sos>` token
- âœ“ Accumulate loss chÃ­nh xÃ¡c

---

### 2.2 HÃ m `evaluate()` (Cell 20)

```python
@torch.no_grad()                    # âœ… No gradient computation
def evaluate(model, iterator, criterion, device):
    model.eval()                    # âœ… Set model to eval mode
    epoch_loss = 0.0
    
    for src, src_lengths, trg in tqdm(iterator, ...):
        src, trg = src.to(device), trg.to(device)
        
        output = model(src, src_lengths, trg, teacher_forcing_ratio=0)
        # âœ… teacher_forcing_ratio=0 â†’ use predictions, not ground truth
        
        output = output[:, 1:, :].reshape(-1, V)
        trg    = trg[:, 1:].reshape(-1)
        
        epoch_loss += criterion(output, trg).item()
    
    return epoch_loss / len(iterator)
```

**âœ… Äiá»ƒm tá»‘t:**
- âœ“ `@torch.no_grad()` - khÃ´ng tÃ­nh gradient (tiáº¿t kiá»‡m memory)
- âœ“ `model.eval()` - táº¯t dropout, batch norm
- âœ“ `teacher_forcing_ratio=0` - Ä‘Ã¡nh giÃ¡ thá»±c táº¿ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n gÃ¬

---

### 2.3 Cáº¥u hÃ¬nh Loss, Optimizer, Scheduler (Cell 20)

```python
PAD_IDX = fr_vocab.stoi["<pad>"]
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)
# âœ… ignore_index trÃ¡nh tÃ­nh loss trÃªn padding tokens

optimizer = optim.Adam(model.parameters(), lr=1e-3)
# âœ… Learning rate = 0.001 (há»£p lÃ½ cho transformer-like model)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", factor=0.5, patience=2
)
# âœ… Giáº£m learning rate náº¿u val loss khÃ´ng cáº£i thiá»‡n

CLIP = 1.0
# âœ… Gradient clipping threshold
```

**âœ… PhÃ¢n tÃ­ch:**
| ThÃ nh pháº§n | GiÃ¡ trá»‹ | Nháº­n xÃ©t |
|-----------|--------|---------|
| Loss | CrossEntropyLoss | âœ… Chuáº©n cho NMT |
| LR | 0.001 | âœ… PhÃ¹ há»£p |
| Optimizer | Adam | âœ… Adaptive learning rate |
| Scheduler | ReduceLROnPlateau | âœ… Adaptive scheduling |
| Grad Clip | 1.0 | âœ… TrÃ¡nh gradient explosion |

---

### 2.4 Training Loop (Cell 22)

```python
N_EPOCHS = 20
PATIENCE = 3  # Early stopping
best_valid_loss = float("inf")
patience_counter = 0

for epoch in range(N_EPOCHS):
    # 1ï¸âƒ£ Training
    train_loss = train_epoch(model, train_loader, optimizer, criterion,
                            clip=CLIP, device=device, tf_ratio=TEACHER_FORCING_RATIO)
    # âœ… Training vá»›i teacher forcing (tá»‰ lá»‡ 0.5)
    
    # 2ï¸âƒ£ Validation
    valid_loss = evaluate(model, val_loader, criterion, device)
    # âœ… Validation khÃ´ng cÃ³ teacher forcing (tá»‰ lá»‡ 0)
    
    # 3ï¸âƒ£ Learning rate scheduling
    scheduler.step(valid_loss)
    # âœ… Äiá»u chá»‰nh learning rate dá»±a trÃªn val loss
    
    # 4ï¸âƒ£ Early stopping
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        patience_counter = 0
        torch.save(model.state_dict(), "best_model.pth")  # âœ… LÆ°u best model
    else:
        patience_counter += 1
        if patience_counter >= PATIENCE:
            break  # âœ… Dá»«ng náº¿u khÃ´ng cáº£i thiá»‡n trong 3 epochs
```

**âœ… Äiá»ƒm tá»‘t:**
- âœ“ Early stopping vá»›i `patience=3`
- âœ“ LÆ°u best model (`best_model.pth`)
- âœ“ Learning rate scheduling
- âœ“ TÃ­nh cáº£ train loss vÃ  val loss

**âš ï¸ CÃ³ thá»ƒ cáº£i thiá»‡n:**
- CÃ³ thá»ƒ thÃªm test loss tracking
- CÃ³ thá»ƒ lÆ°u checkpoint má»—i epoch cho debug

---

## III. PHÃ‚N TÃCH KIáº¾N TRÃšC MODEL

### 3.1 Encoder (Cell 14)

```python
class Encoder(nn.Module):
    def forward(self, src, src_lengths, return_outputs=False):
        embedded = self.dropout(self.embedding(src))  # [B, T, emb_dim]
        
        # âœ… Pack padded sequence (bá» padding)
        packed = pack_padded_sequence(embedded, src_lengths, 
                                     batch_first=True, enforce_sorted=True)
        
        packed_output, (hidden, cell) = self.lstm(packed)
        
        # âœ… Unpack náº¿u cáº§n cho attention
        if return_outputs:
            encoder_outputs, _ = pad_packed_sequence(packed_output, batch_first=True)
            return hidden, cell, encoder_outputs
        
        return hidden, cell, None
```

**âœ… Äiá»ƒm tá»‘t:**
- âœ“ Sá»­ dá»¥ng `pack_padded_sequence` (hiá»‡u quáº£ hÆ¡n)
- âœ“ Return `encoder_outputs` cho attention
- âœ“ Dropout trÃªn embedding

---

### 3.2 Decoder (Cell 16)

```python
class Decoder(nn.Module):
    def forward(self, input, hidden, cell):
        input = input.unsqueeze(1)  # [B] â†’ [B, 1]
        embedded = self.dropout(self.embedding(input))
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.fc_out(output.squeeze(1))  # [B, vocab_size]
        return prediction, hidden, cell
```

**âœ… Äiá»ƒm tá»‘t:**
- âœ“ Step-by-step decoding
- âœ“ Maintains hidden state

---

### 3.3 Seq2Seq (Cell 18)

```python
class Seq2Seq(nn.Module):
    def forward(self, src, src_lengths, trg, teacher_forcing_ratio=0.5):
        hidden, cell, _ = self.encoder(src, src_lengths)
        
        inp = trg[:, 0]  # <sos> token
        for t in range(1, trg_len):
            out, hidden, cell = self.decoder(inp, hidden, cell)
            
            # âœ… Teacher forcing
            if random.random() < teacher_forcing_ratio:
                inp = trg[:, t]  # Use ground truth
            else:
                inp = out.argmax(1)  # Use prediction
```

**âœ… Äiá»ƒm tá»‘t:**
- âœ“ Chuáº©n teacher forcing
- âœ“ Support inference (ratio=0)

---

## IV. HYPERPARAMETERS SUMMARY

| Tham sá»‘ | GiÃ¡ trá»‹ | Má»¥c Ä‘Ã­ch |
|---------|--------|---------|
| **Input/Output Dim** | 10000 / 10000 | Vocab size |
| **Embedding Dim** | 256 | Token embedding dimension |
| **Hidden Dim** | 512 | LSTM hidden state size |
| **LSTM Layers** | 2 | Sá»‘ stacked LSTM layers |
| **Dropout** | 0.4 | Regularization |
| **Batch Size** | 64 | Mini-batch size |
| **Learning Rate** | 0.001 | Adam learning rate |
| **Teacher Forcing** | 0.5 | Tá»‰ lá»‡ dÃ¹ng ground truth |
| **Gradient Clip** | 1.0 | Clip grad norm |
| **Scheduler** | ReduceLROnPlateau | Adaptive LR |
| **Patience (Early Stop)** | 3 | Epochs khÃ´ng cáº£i thiá»‡n |
| **Max Epochs** | 20 | Max training epochs |

---

## V. RÃšT RA ÄÆ¯á»¢C GÃŒ Tá»ª NOTEBOOK?

### 5.1 Kiáº¿n thá»©c NMT (Neural Machine Translation)

1. **Seq2Seq Architecture**: Encoder-Decoder vá»›i LSTM
2. **Teacher Forcing**: Accelerate training nhÆ°ng cÃ³ exposure bias
3. **Attention Mechanism**: Giáº£i quyáº¿t bottleneck cá»§a context vector
4. **BLEU Score**: Metric Ä‘Ã¡nh giÃ¡ translation quality
5. **Early Stopping**: TrÃ¡nh overfitting

### 5.2 Ká»¹ thuáº­t PyTorch

1. **Pack/Unpack**: Xá»­ lÃ½ variable-length sequences hiá»‡u quáº£
2. **Gradient Clipping**: TrÃ¡nh exploding gradient
3. **Learning Rate Scheduling**: Adaptive learning rate
4. **Checkpoint Saving**: LÆ°u best model
5. **Teacher Forcing**: Dynamic training strategy

### 5.3 Best Practices

| Ká»¹ thuáº­t | Lá»£i Ã­ch |
|---------|--------|
| `model.train()` / `model.eval()` | Äiá»u khiá»ƒn dropout/batchnorm |
| `@torch.no_grad()` | Tiáº¿t kiá»‡m memory, tá»‘c Ä‘á»™ nhanh |
| `ignore_index` trong loss | Bá» qua padding tokens |
| `clip_grad_norm_()` | TrÃ¡nh gradient explosion |
| Early stopping | TrÃ¡nh overfitting |
| Learning rate scheduling | Converge tá»‘t hÆ¡n |

---

## VI. CÃ“ CHUáº¨N KHÃ”NG?

### âœ… **CÃ“ CHUáº¨N (Äiá»ƒm tá»‘t)**

1. **Loss function**: ÄÃºng (CrossEntropyLoss + ignore_index)
2. **Optimizer**: Há»£p lÃ½ (Adam + scheduler)
3. **Training loop**: Chuáº©n (train/eval, early stopping)
4. **Gradient clipping**: CÃ³ (CLIP = 1.0)
5. **Checkpoint saving**: CÃ³
6. **Teacher forcing**: Chuáº©n (tá»‰ lá»‡ 0.5)
7. **Data handling**: Tá»‘t (padding, sorting by length)

### âš ï¸ **CÃ“ THá»‚ Cáº¢I THIá»†N**

1. **Validation frequency**: Chá»‰ validate 1 láº§n/epoch
   - CÃ³ thá»ƒ validate má»—i K batch Ä‘á»ƒ catch overfitting sá»›m hÆ¡n

2. **Metrics logging**: Chá»‰ log loss
   - CÃ³ thá»ƒ log thÃªm BLEU, accuracy trÃªn validation

3. **Hyperparameter tuning**: ChÆ°a cÃ³ ablation study
   - CÃ³ thá»ƒ thá»­ cÃ¡c tá»‰ lá»‡ teacher forcing khÃ¡c
   - CÃ³ thá»ƒ thá»­ learning rate khÃ¡c

4. **Batch size handling**: Fixed batch size
   - CÃ³ thá»ƒ dÃ¹ng dynamic batching Ä‘á»ƒ tá»‘i Æ°u GPU memory

5. **Data shuffling**: Shuffle train set nhÆ°ng khÃ´ng validation/test
   - âœ… ÄÃºng (nhÆ°ng comment thÃªm Ä‘á»ƒ rÃµ)

6. **Random seed**: ChÆ°a set seed
   - NÃªn set `torch.manual_seed()` Ä‘á»ƒ reproducibility

---

## VII. KIáº¾N THá»¨C CORE Tá»ªNG CELL

| Cell | Ná»™i dung | Há»c Ä‘Æ°á»£c gÃ¬ |
|------|---------|------------|
| 4 | Setup dependencies | PyTorch installation, device management |
| 6 | Download dataset | Data loading tá»« URL, gzip handling |
| 8 | Tokenization | spaCy tokenizer, preprocessing |
| 10 | Vocabulary building | Frequency-based vocab, numericalization |
| 12 | Dataset & DataLoader | Custom Dataset class, collate_fn |
| 14 | Encoder | pack_padded_sequence, LSTM |
| 16 | Decoder | Step-by-step decoding |
| 18 | Seq2Seq | Teacher forcing, inference |
| **20** | **Training setup** | **Loss, optimizer, scheduler** |
| **22** | **Training loop** | **Early stopping, checkpoint** |
| 24 | Visualization | Matplotlib, training curves |
| 28 | Inference | Greedy decoding |
| 30 | BLEU score | nltk bleu_score |
| 40 | Attention | Attention mechanism |
| 42 | DecoderWithAttention | Attention + decoder |
| 44 | Seq2SeqWithAttention | Full attention model |

---

## Káº¾T LUáº¬N

**âœ… Code training CÃ“ CHUáº¨N!**

Notebook nÃ y lÃ m tá»‘t:
- âœ“ Chuáº©n architecture (Encoder-Decoder)
- âœ“ Chuáº©n training loop
- âœ“ Chuáº©n hyperparameters
- âœ“ CÃ³ early stopping & checkpoint
- âœ“ CÃ³ learning rate scheduling

**CÃ³ thá»ƒ upgrade:**
- ThÃªm metrics tracking (BLEU on validation)
- ThÃªm reproducibility (set seed)
- ThÃªm ablation studies
- ThÃªm validation frequency

---

